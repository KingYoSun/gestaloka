# AI応答時間最適化実装レポート

## 日付: 2025年6月30日

## 概要
GM AI評議会の応答時間短縮を目的として、AIエージェントの種類に応じて使用するLLMモデルを切り替える戦略を実装しました。

## 実装内容

### 1. 環境変数の追加
- `.env`ファイルに軽量モデルと標準モデルの設定を追加
  - `GEMINI_MODEL_FAST=gemini-2.5-flash`（軽量モデル）
  - `GEMINI_MODEL_STANDARD=gemini-2.5-pro`（標準モデル）

### 2. モデル選択システムの実装
- `app/services/ai/model_types.py`: AIエージェントタイプとモデルタイプの定義
- `app/services/ai/gemini_factory.py`: モデル選択とクライアント管理のファクトリー
- `GeminiClient`クラスの拡張: モデルタイプ指定機能を追加

### 3. AIエージェント別のモデル割り当て

#### 軽量モデル（gemini-2.5-flash）使用
- **状態管理AI**: ルール判定、JSON形式レスポンス（temperature: 0.3）
- **混沌AI**: イベント発生判定（temperature: 0.95）
- **世界の意識AI**: 状態更新、トリガーチェック（temperature: 0.3）

#### 標準モデル（gemini-2.5-pro）使用
- **脚本家AI**: 物語生成（temperature: 0.8）
- **NPC管理AI**: キャラクター生成（temperature: 0.8）
- **歴史家AI**: ログ編纂（temperature: 0.5）

### 4. 処理フローの最適化
- タスクジェネレーターの依存関係により、状態管理AI → 脚本家AIの順序を保証
- 矛盾のない物語生成を実現
- パフォーマンスログの追加により、各エージェントの実行時間を可視化

### 5. 実装ファイルの更新
- `app/services/game_session.py`: ファクトリーを使用したエージェント初期化
- `app/services/ai/dispatch_simulator.py`: 派遣シミュレーターの更新
- `app/services/ai/dispatch_interaction.py`: 相互作用マネージャーの更新
- `app/ai/coordinator.py`: パフォーマンスログの追加

## 期待される効果

### パフォーマンス改善
- **現在**: 約20秒
- **目標**: 10-15秒（25-50%削減）

### 改善要因
1. 軽量モデルによる高速な判定処理
2. 適切なtemperature設定による処理の最適化
3. モデル別のクライアントキャッシュ
4. 詳細なパフォーマンスログによる継続的な改善

## 技術的な利点
- 環境変数による柔軟なモデル切り替え
- 開発/本番環境での異なる設定が可能
- エージェントごとの最適なモデル選択
- 処理の整合性を保ちながら高速化を実現

## 次のステップ
1. 実際のパフォーマンス測定とチューニング
2. 各エージェントの実行時間分析
3. さらなる最適化ポイントの特定
4. 必要に応じてtemperature値の調整

## 関連ファイル
- `/backend/app/services/ai/model_types.py`
- `/backend/app/services/ai/gemini_factory.py`
- `/backend/app/services/ai/gemini_client.py`
- `/backend/app/core/config.py`
- `/.env`